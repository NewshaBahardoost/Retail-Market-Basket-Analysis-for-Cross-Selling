{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ID: V01053626\n",
        "Name: Newsha Bahardoost"
      ],
      "metadata": {
        "id": "3pBeCuanb_Rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "\n",
        "def read_data(file_path):\n",
        "    \"\"\"Reads basket data from a file, each line represents a transaction.\"\"\"\n",
        "    baskets = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            session = set(line.strip().split())  # Read items as a set\n",
        "            baskets.append(session)\n",
        "    return baskets\n",
        "\n",
        "def generate_frequent_itemsets(baskets, prev_frequent, k, min_support=100):\n",
        "    \"\"\"Generates frequent itemsets of size k using the Apriori principle.\"\"\"\n",
        "    item_counts = defaultdict(int)\n",
        "\n",
        "    # Generate candidate itemsets\n",
        "    for basket in baskets:\n",
        "        for itemset in combinations(sorted(basket), k):\n",
        "            if k == 1 or all(tuple(sub) in prev_frequent for sub in combinations(itemset, k-1)):\n",
        "                item_counts[itemset] += 1\n",
        "\n",
        "    # Prune itemsets that do not meet minimum support threshold\n",
        "    return {itemset: count for itemset, count in item_counts.items() if count >= min_support}\n",
        "\n",
        "def calculate_confidence(numerator, denominator):\n",
        "    \"\"\"Calculates confidence for association rules.\"\"\"\n",
        "    return numerator / denominator if denominator > 0 else 0\n",
        "\n",
        "def generate_rules(frequent_itemsets, prev_frequent, k):\n",
        "    \"\"\"Generates association rules from frequent itemsets.\"\"\"\n",
        "    rules = []\n",
        "\n",
        "    for itemset, count in frequent_itemsets.items():\n",
        "        for antecedent in combinations(itemset, k-1):\n",
        "            antecedent = tuple(sorted(antecedent))\n",
        "            consequent = tuple(sorted(set(itemset) - set(antecedent)))\n",
        "\n",
        "            if antecedent in prev_frequent:\n",
        "                conf = calculate_confidence(count, prev_frequent[antecedent])\n",
        "                rules.append((antecedent, consequent, conf))\n",
        "\n",
        "    return sorted(rules, key=lambda x: (-x[2], x[0], x[1]))\n",
        "\n",
        "def main():\n",
        "    baskets = read_data('/content/p2-baskets.txt')\n",
        "\n",
        "    # Generate frequent itemsets\n",
        "    L1 = generate_frequent_itemsets(baskets, None, 1, min_support=100)\n",
        "    print(f\"Number of frequent items in L1: {len(L1)}\")\n",
        "\n",
        "    L2 = generate_frequent_itemsets(baskets, set(L1.keys()), 2, min_support=100)\n",
        "    print(f\"Number of frequent itemsets in L2: {len(L2)}\")\n",
        "\n",
        "    L3 = generate_frequent_itemsets(baskets, set(L2.keys()), 3, min_support=100)\n",
        "    print(f\"Number of frequent itemsets in L3: {len(L3)}\")\n",
        "\n",
        "    # Generate and sort rules\n",
        "    pair_rules = generate_rules(L2, L1, 2)[:5]\n",
        "    triplet_rules = generate_rules(L3, L2, 3)[:3]\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nTop 5 Pair Rules:\")\n",
        "    for rule in pair_rules:\n",
        "        print(f\"{' '.join(rule[0])} -> {' '.join(rule[1])}: {rule[2]:.5f}\")\n",
        "\n",
        "    print(\"\\nTop 3 Triplet Rules:\")\n",
        "    for rule in triplet_rules:\n",
        "        print(f\"{', '.join(rule[0])} -> {rule[1][0]}: {rule[2]:.5f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI7XN0Sr3y8F",
        "outputId": "0801e2d4-16eb-4dad-8292-77c7ad525253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of frequent items in L1: 647\n",
            "Number of frequent itemsets in L2: 1334\n",
            "Number of frequent itemsets in L3: 233\n",
            "\n",
            "Top 5 Pair Rules:\n",
            "DAI93865 -> FRO40251: 1.00000\n",
            "GRO85051 -> FRO40251: 0.99918\n",
            "GRO38636 -> FRO40251: 0.99065\n",
            "ELE12951 -> FRO40251: 0.99057\n",
            "DAI88079 -> FRO40251: 0.98673\n",
            "\n",
            "Top 3 Triplet Rules:\n",
            "DAI23334, ELE92920 -> DAI62779: 1.00000\n",
            "DAI31081, GRO85051 -> FRO40251: 1.00000\n",
            "DAI55911, GRO85051 -> FRO40251: 1.00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "\n",
        "def read_data(file_path):\n",
        "    \"\"\"Reads basket data from a file, each line represents a transaction.\"\"\"\n",
        "    baskets = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            session = set(line.strip().split())  # Read items as a set\n",
        "            baskets.append(session)\n",
        "    return baskets\n",
        "\n",
        "def generate_frequent_itemsets(baskets, prev_frequent, k, min_support=100):\n",
        "    \"\"\"Generates frequent itemsets of size k using the Apriori principle.\"\"\"\n",
        "    item_counts = defaultdict(int)\n",
        "\n",
        "    # Generate candidate itemsets\n",
        "    for basket in baskets:\n",
        "        for itemset in combinations(sorted(basket), k):\n",
        "            if k == 1 or all(tuple(sub) in prev_frequent for sub in combinations(itemset, k-1)):\n",
        "                item_counts[itemset] += 1\n",
        "\n",
        "    # Prune itemsets that do not meet minimum support threshold\n",
        "    return {itemset: count for itemset, count in item_counts.items() if count >= min_support}\n",
        "\n",
        "def calculate_confidence(numerator, denominator):\n",
        "    \"\"\"Calculates confidence for association rules.\"\"\"\n",
        "    return numerator / denominator if denominator > 0 else 0\n",
        "\n",
        "def generate_rules(frequent_itemsets, prev_frequent, k):\n",
        "    \"\"\"Generates association rules from frequent itemsets.\"\"\"\n",
        "    rules = []\n",
        "\n",
        "    for itemset, count in frequent_itemsets.items():\n",
        "        for antecedent in combinations(itemset, k-1):\n",
        "            antecedent = tuple(sorted(antecedent))\n",
        "            consequent = tuple(sorted(set(itemset) - set(antecedent)))\n",
        "\n",
        "            if antecedent in prev_frequent:\n",
        "                conf = calculate_confidence(count, prev_frequent[antecedent])\n",
        "                rules.append((antecedent, consequent, conf))\n",
        "\n",
        "    return sorted(rules, key=lambda x: (-x[2], x[0], x[1]))\n",
        "\n",
        "# Read data and generate frequent itemsets for Task 1\n",
        "baskets = read_data('/content/p2-baskets.txt')\n",
        "L1 = generate_frequent_itemsets(baskets, None, 1, min_support=100)\n",
        "L2 = generate_frequent_itemsets(baskets, set(L1.keys()), 2, min_support=100)\n",
        "\n",
        "# Generate and filter pair rules for Task 1 (confidence >= 0.985)\n",
        "pair_rules = generate_rules(L2, L1, 2)\n",
        "pair_rules_filtered = [rule for rule in pair_rules if rule[2] >= 0.985]\n",
        "top_pair_rules = sorted(pair_rules_filtered, key=lambda x: (-x[2], x[0], x[1]))[:5]\n",
        "\n",
        "# Print Task 1 output\n",
        "for rule in top_pair_rules:\n",
        "    print(f\"{' '.join(rule[0])} -> {' '.join(rule[1])}: {rule[2]:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujT5bK0Qbe0q",
        "outputId": "b3b75544-d6b6-43f9-fa92-6fb64aba5b25"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DAI93865 -> FRO40251: 1.00000\n",
            "GRO85051 -> FRO40251: 0.99918\n",
            "GRO38636 -> FRO40251: 0.99065\n",
            "ELE12951 -> FRO40251: 0.99057\n",
            "DAI88079 -> FRO40251: 0.98673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate frequent itemsets for Task 2\n",
        "L3 = generate_frequent_itemsets(baskets, set(L2.keys()), 3, min_support=100)\n",
        "\n",
        "# Generate and filter triplet rules for Task 2 (confidence == 1.0)\n",
        "triplet_rules = generate_rules(L3, L2, 3)\n",
        "triplet_rules_filtered = [rule for rule in triplet_rules if rule[2] == 1.0]\n",
        "top_triplet_rules = sorted(triplet_rules_filtered, key=lambda x: (-x[2], x[0], x[1]))[:5]\n",
        "\n",
        "# Print Task 2 output\n",
        "for rule in top_triplet_rules:\n",
        "    print(f\"{' '.join(rule[0])} -> {' '.join(rule[1])}: {rule[2]:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxyC8aHYbrwO",
        "outputId": "c6b70cf4-33c7-47b9-d9a0-06e4ce2d47b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DAI23334 ELE92920 -> DAI62779: 1.00000\n",
            "DAI31081 GRO85051 -> FRO40251: 1.00000\n",
            "DAI55911 GRO85051 -> FRO40251: 1.00000\n",
            "DAI62779 DAI88079 -> FRO40251: 1.00000\n",
            "DAI75645 GRO85051 -> FRO40251: 1.00000\n"
          ]
        }
      ]
    }
  ]
}